---
title: "Analysis"
author: "Rodrigo Malagón"
date: "2024-08-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pollution concentration modelling in Castellón de la Plana for 2023 using LSTM

This project aims to adapt the LSTM framework to the modellinf of pollutants in the city of Castellón de la Plana, which

```{r}
#install.packages("keras3")
library(keras3) 
```

## Data Inspection and Processing

Retrieve datasets

```{r}
# Retrieve pollution datasets
data_path <- './data/raw_data/'
file_names <- list.files(data_path, full.names = FALSE, pattern = "MDEST*") #one file per station
```

```{r}
# Retrieve sattions metadata file
stations <- read.csv(paste0(data_path,'stations.csv'))
stations
```

Review length of datasets per station to have an initial idea of missing data across stations.

```{r}
station_id <- list()
len <- list()
for(file in file_names){
  df <- read.csv(paste0(data_path,file),skip = 3,sep ="\t",header=FALSE)
  station_id <- c(station_id,substr(file,6,13))
  len <- c(len,dim(df)[1]-2)
  #print(paste0('Number of rows for station with id ',station_id,': ',dim(df)[1]-2))
}
station_id |> unlist() -> station_id
len |> unlist() -> len
df <- data.frame('station_id' = station_id,'number_of_days' = len)
df <- merge(df,stations,by = 'station_id')
df
```

Create list with all pollution dataframes per station

```{r}
pollution_data <- list()
for(file in file_names){
  id <- substr(file,6,13)
  df <- read.csv(paste0(data_path,file),skip = 3,sep ="\t",header=FALSE)
  colnames(df)<-df[1,]
  df <- df[3:dim(df)[1],]
  df$station_id <- id
  pollution_data[[id]] <- df
}
```

Detecting the pollutants with greater representation across stations

```{r}
pollutants <- c('PM10','PM2.5','NOx','NO2','SO2')
num_stations <- list()
for(pol in pollutants){
  num <- lapply(pollution_data, function(x){pol %in%  colnames(x)}) |> unlist() |> sum()  
  num_stations <- c(num_stations,num)
}
num_stations|> unlist()-> num_stations
data.frame(pollutants,num_stations)
```

Selecting a station dataset to work individually and inspect missing values

```{r}
id <- '12040010'
pol <- pollution_data[[id]]

# Change data type of variables
change_type <- function(series){
  r <- series
  lapply(r,function(x){
      gsub(x,pattern = ',',replacement = '.')
    }) |> unlist() |> as.numeric() -> r
  return(r)
}

vars <- c('PM2.5','H.Rel.','R.Sol.','Veloc.','Temp.','Direc.')
for(var in vars){
  change_type(pol[[var]]) -> pol[[var]]
}

#Selecting variables
pol <- pol[vars]
head(pol[vars])

# save station dataset
#dir <- './processed_data/'
#data_file_path <- paste0(dir,'station_',id,'.rda')
#save(pol,file = data_file_path)
```

Filling missing values

```{r}
load('./processed_data/station_12040010.rda')
plot(1:365,pol$PM2.5,
     type = 'l',
     col = 'blue',
     main = 'Series with gaps',
     ylab = 'PM 2.5 concentration'
     )
```

```{r}
# Functions to replace missing values with mean of remaining values
mean_replace <- function(arg){#requires a numeric vector as argument
  vec <- arg
  
  # Compute mean value for the present values
  vec[!is.na(vec)]|> mean() |> round(2) -> m
  
  # Do replacement
  replacement_counter <- 0
  replaced_values <- rep(NA,length(vec)) # vector catching just replaced values
  for(i in 1:length(vec)){
    if(is.na(vec[[i]])){
      replacement_counter <- replacement_counter + 1
      vec[[i]] <- m
      replaced_values[[i]] <- m
    }
  }
  print(paste0('Percentage of missing values replaced with mean values for series: ',round(replacement_counter/length(vec),2),'%'))
  return(list(new_series = vec,replaced_series = replaced_values))
}

apply_replacement_to_df <- function(df,col_names_vector){
  new_df <- df
  replacement_df <- df[col_names_vector]
  for(name in col_names_vector){
    ret <- mean_replace(new_df[[name]])
    new_df[[name]] <- ret$new_series
    replacement_df [[name]] <- ret$replaced_series
  }
  return(list(completed_df=new_df,replacement_df=replacement_df))
}

```

Application of missing values filling

```{r}
pol_complete <- apply_replacement_to_df(pol,vars)
pol_complete$completed_df |> head()
pol <- pol_complete$completed_df
```

Plot time series with replaced values

```{r}
plot(1:365,pol$PM2.5,
     type = 'l',
     col = 'blue'
     )
lines(1:365,pol_complete$replacement_df$PM2.5,
       col = 'red',
       lwd = 2)

# Legend
legend("topright", legend = c('completed series','replaced values'), col = c('blue','red'), lty = 1, lwd = 2)
```

## LSTM processing

Preparing dataset for LSTM

```{r}
# Data selection
data <- pol |> data.matrix() |> unname()
ndata <- dim(data)[2]
len_series <- dim(data)[1]
```

Dataset normalization

```{r}
# Min-max normalization
min_max_norm <- function(series){
  m <- min(series)
  d <- max(series)-m
  return((series-m)/d)
}

# Z_score normalization
z_norm <- function(series){
  m <- mean(series)
  s <- sd(series)
  r <- (series-m)/s
  return(r)
}


for(col in 1:ndata){
  data[,col] <- z_norm(data[,col])
}

t_series <- data[,1]
```

Creation of LSTM-ready datasets

```{r}
lag <- 20
pred_window <- 5

# Apply lag to dataset to obtain array of input sequences
X <- array(NA, dim = c(len_series - (lag + pred_window) + 1, lag, ndata))
len <- dim(X)[1]

y <- array(NA, dim = c(len, pred_window))




for(i in 1:len){
  lag_selection_1 <- i:(i + (lag-1))
  X[i,,1] <- t_series[lag_selection_1]
  if(ndata > 1){
    lag_selection_2 <- lag_selection_1 + 1
    X[i,,2:ndata] <- data[lag_selection_2,2:ndata]
  }
  y_selection <- (i+lag):(i + lag + pred_window -1)
  y[i,]<- t_series[y_selection]
}



# Split into training and test sets
train_size <- round(len * 0.8)
X_train <- X[1:train_size,,]
y_train <- y[1:train_size,]

X_test <- X[(train_size + 1):len,,]
y_test <- y[(train_size + 1):len,]
```

A little of visualization of our datasets

```{r}
plot((1+lag):(len+lag),y[,1],
      col = 'grey',
        type='l',
      lty=5,
        xlab = 'Day of the year',
        ylab = 'Concentration of PM2.5 (normalized)',
        xlim = c(1,365),
     lwd =1,
     main='Data split for two samples at time indices t=1, 100'
     )
for(id in c(1, 100)){
  lines(id:(lag+id-1),t_series[id:(lag+id-1)],
      col = 'black',lwd = 1)
points((id+lag),y[id,1],
       col = 'red',
       lwd = 2)
}



# Legend
legend("topright", legend = c("X[*,,1]",'y','y[*]'), col = c("black", "gray",'red'), lty = 1, lwd = 2)
```

#Data modelling

```{r}
model <- 0
model <- keras_model_sequential() %>%
  layer_lstm(units = lag, activation = 'relu', input_shape = c(lag, ndata)) %>%
  layer_dense(units = pred_window)

#Set parameters
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(learning_rate = 1e-4),
  metrics = c('mae')
)


summary(model)
```

## Train model

```{r}
set.seed(1000)

history <- model %>% fit(
  x = X_train, 
  y = y_train,
  epochs = 15,
  batch_size = 1,
  validation_data = list(X_test, y_test),
  verbose = 2
)
```

Save model
```{r}
#Sys.Date() |> as.character() -> date
#metric <- 'mae'
#(model_name <- paste0('lstm_model_',metric,'_',date,'.h5'))
#(path <- paste0('./models/',model_name))
#save_model(history,filepath = path)
```


## Predict and bumb-up

Predict
```{r}
predictions <- model  %>% predict(X_test)
head(predictions)
```

```{r}
# Bump-up definition to retrieve original data ranges
bump_up_min_max <- function(series,ref_series){
  s <- min(ref_series)
  d <- max(ref_series)-s
  return(series*d + s)
}

bump_up_z_score <- function(series,ref_series){
  m <- mean(ref_series)
  s <- sd(ref_series)
  return(series*s + m)
}
```


### One-day-ahead prediction


Plot one-day-ahead predictions

```{r}
# Plot the results
actual <- bump_up_z_score(y_test[,1],pol$PM2.5)
predicted <- bump_up_z_score(predictions[,1],pol$PM2.5)  


title = paste0("One-day ahead predictions (fixed lag window of ",lag,' days)')

plot(x = (train_size + lag + 1):(365 - pred_window + 1),
     y = actual,
     type = 'l',
     col = 'blue',
     lwd = 2,
     main = title,
     xlab = "Day of the year",
     ylab = "PM2.5 concentration")
lines(x = (train_size + lag + 1):(365 - pred_window + 1),
      y = predicted,
      col = 'red',
      lwd = 2)


legend("topright", legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2)
```


One-day-ahead residuals tests

```{r}
residuals <- predicted-actual
residuals |> hist(probability = TRUE,
                  breaks = round(length(residuals)/5),
                  col = 'azure2')
lines(density(residuals),col = 'cyan4',lwd=2)
```

Shapiro-Wilk normality test
```{r}
shapiro.test(residuals)
```


### Present prediction using full time window